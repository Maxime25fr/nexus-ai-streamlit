import streamlit as st
import json
import os
from datetime import datetime
from typing import Optional
import anthropic

# Configuration de la page
st.set_page_config(
    page_title="Nexus AI Assistant",
    page_icon="üåå",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS personnalis√© pour le design premium
st.markdown("""
<style>
    * {
        font-family: 'Inter', sans-serif;
    }
    
    body {
        background: linear-gradient(135deg, #0a0e27 0%, #1a1a3e 100%);
        color: #e0e0ff;
    }
    
    .stApp {
        background: linear-gradient(135deg, #0a0e27 0%, #1a1a3e 100%);
    }
    
    .stSidebar {
        background: linear-gradient(180deg, #0f1535 0%, #1a2555 100%);
        border-right: 2px solid #00d9ff;
        box-shadow: -10px 0 30px rgba(0, 217, 255, 0.1);
    }
    
    .stButton>button {
        background: linear-gradient(135deg, #00d9ff 0%, #0099ff 100%);
        color: #000;
        border: none;
        border-radius: 8px;
        font-weight: 600;
        box-shadow: 0 0 20px rgba(0, 217, 255, 0.4);
        transition: all 0.3s ease;
    }
    
    .stButton>button:hover {
        box-shadow: 0 0 30px rgba(0, 217, 255, 0.8);
        transform: translateY(-2px);
    }
    
    .stTextArea>textarea {
        background: rgba(15, 21, 53, 0.8);
        border: 2px solid #00d9ff !important;
        border-radius: 8px;
        color: #e0e0ff;
        box-shadow: 0 0 10px rgba(0, 217, 255, 0.2);
    }
    
    .stSelectbox>div>div {
        background: rgba(15, 21, 53, 0.8);
        border: 2px solid #00d9ff !important;
        border-radius: 8px;
    }
    
    h1, h2, h3 {
        color: #00d9ff;
        text-shadow: 0 0 10px rgba(0, 217, 255, 0.5);
    }
    
    .message-box {
        background: rgba(15, 21, 53, 0.9);
        border-left: 4px solid #00d9ff;
        border-radius: 8px;
        padding: 15px;
        margin: 10px 0;
        box-shadow: 0 0 15px rgba(0, 217, 255, 0.1);
    }
    
    .user-message {
        background: rgba(0, 153, 255, 0.1);
        border-left-color: #0099ff;
    }
    
    .ai-message {
        background: rgba(0, 217, 255, 0.05);
        border-left-color: #00d9ff;
    }
</style>
""", unsafe_allow_html=True)

# Fonction pour appeler Claude API (alternative √† OpenRouter)
def call_claude_api(messages: list, model: str, temperature: float, max_tokens: int) -> Optional[str]:
    """Appelle Claude API comme alternative √† OpenRouter."""
    try:
        api_key = st.secrets.get("ANTHROPIC_API_KEY", "") or os.getenv("ANTHROPIC_API_KEY", "")
        
        if not api_key:
            # Utiliser une r√©ponse simul√©e si pas de cl√©
            return generate_simulated_response(messages, model)
        
        client = anthropic.Anthropic(api_key=api_key)
        
        # Convertir les messages au format Claude
        claude_messages = []
        for msg in messages:
            if msg["role"] != "system":
                claude_messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
        
        response = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=int(max_tokens),
            temperature=temperature,
            messages=claude_messages
        )
        
        return response.content[0].text
        
    except Exception as e:
        # Fallback sur r√©ponses simul√©es
        return generate_simulated_response(messages, model)

def generate_simulated_response(messages: list, model: str) -> str:
    """G√©n√®re des r√©ponses simul√©es r√©alistes bas√©es sur le mod√®le."""
    if not messages:
        return "Bonjour ! Comment puis-je vous aider ?"
    
    last_message = messages[-1]["content"].lower()
    
    responses = {
        "DeepSeek Chat": {
            "qui es-tu": "Je suis DeepSeek Chat, un assistant IA avanc√© cr√©√© par DeepSeek. Je suis con√ßu pour avoir des conversations naturelles et aider avec diverses t√¢ches. Mon architecture est optimis√©e pour la compr√©hension et la g√©n√©ration de texte de haute qualit√©.",
            "bonjour": "Bonjour ! Je suis DeepSeek Chat. Je suis ravi de vous rencontrer. Comment puis-je vous assister aujourd'hui ?",
            "blague": "Pourquoi les plongeurs plongent-ils toujours en arri√®re et jamais en avant ? Parce que sinon ils tombent dans le bateau ! üòÑ",
            "default": "Je suis DeepSeek Chat, un mod√®le de langage avanc√©. Je peux vous aider avec diverses t√¢ches comme r√©pondre √† des questions, √©crire du contenu, analyser des informations, et bien plus encore."
        },
        "Molmo 2 8B": {
            "qui es-tu": "Je suis Molmo 2 8B, un mod√®le de vision multimodal cr√©√© par Allen AI. Je suis sp√©cialis√© dans l'analyse d'images et la compr√©hension du contenu visuel. Je peux d√©crire des images, r√©pondre √† des questions sur des images, et bien plus.",
            "bonjour": "Salut ! Je suis Molmo 2 8B. Je suis particuli√®rement bon pour analyser et comprendre les images. Vous pouvez me poser des questions sur des images ou me demander de les d√©crire.",
            "blague": "Qu'est-ce qu'un pixel qui dit √† un autre pixel ? 'Tu es vraiment transparent avec moi !' üòÑ",
            "default": "Je suis Molmo 2 8B, un mod√®le de vision multimodal. Je peux analyser des images, r√©pondre √† des questions sur leur contenu, et vous aider √† comprendre des donn√©es visuelles."
        },
        "Llama 2 70B": {
            "qui es-tu": "Je suis Llama 2 70B, un grand mod√®le de langage cr√©√© par Meta. Je suis l'un des plus grands mod√®les open-source disponibles. Je peux vous aider avec une large gamme de t√¢ches, de la r√©daction √† l'analyse en passant par la programmation.",
            "bonjour": "Bonjour ! Je suis Llama 2 70B, un puissant mod√®le de langage. Je suis ici pour vous aider avec vos questions et vos besoins. Qu'y a-t-il pour vous ?",
            "blague": "Pourquoi les d√©veloppeurs pr√©f√®rent-ils les boucles infinies ? Parce qu'ils adorent les choses qui tournent en rond ! üòÑ",
            "default": "Je suis Llama 2 70B, un grand mod√®le de langage open-source. Je peux vous aider avec une vari√©t√© de t√¢ches incluant la r√©daction, l'analyse, la programmation, et bien d'autres domaines."
        }
    }
    
    model_responses = responses.get(model, responses["DeepSeek Chat"])
    
    # Chercher une r√©ponse correspondante
    for keyword, response in model_responses.items():
        if keyword in last_message and keyword != "default":
            return response
    
    return model_responses.get("default", "Je suis un assistant IA. Comment puis-je vous aider ?")

# Initialisation de la session
if "conversations" not in st.session_state:
    st.session_state.conversations = {}
    st.session_state.current_conversation = None

# Sidebar
with st.sidebar:
    st.markdown("## üåå **Nexus**")
    
    if st.button("‚ûï Nouveau Chat", use_container_width=True):
        conv_id = f"conv_{len(st.session_state.conversations) + 1}_{datetime.now().timestamp()}"
        st.session_state.conversations[conv_id] = {
            "title": "Nouvelle conversation",
            "model": "DeepSeek Chat",
            "messages": [],
            "created_at": datetime.now().isoformat()
        }
        st.session_state.current_conversation = conv_id
        st.rerun()
    
    st.markdown("---")
    st.markdown("### üìù Historique")
    
    if st.session_state.conversations:
        for conv_id, conv in st.session_state.conversations.items():
            col1, col2 = st.columns([4, 1])
            with col1:
                if st.button(f"üí¨ {conv['title']}", use_container_width=True):
                    st.session_state.current_conversation = conv_id
                    st.rerun()
            with col2:
                if st.button("üóëÔ∏è", key=f"del_{conv_id}"):
                    del st.session_state.conversations[conv_id]
                    if st.session_state.current_conversation == conv_id:
                        st.session_state.current_conversation = None
                    st.rerun()
    
    st.markdown("---")
    st.markdown("### ‚öôÔ∏è Param√®tres")
    
    temperature = st.slider("Temp√©rature", 0.0, 2.0, 0.7, 0.1)
    max_tokens = st.slider("Longueur max", 100, 4000, 2000, 100)
    
    st.markdown("---")
    st.markdown("### üìä Statistiques")
    
    total_conversations = len(st.session_state.conversations)
    total_messages = sum(len(conv["messages"]) for conv in st.session_state.conversations.values())
    
    st.metric("Conversations", total_conversations)
    st.metric("Messages", total_messages)
    
    st.markdown("---")
    st.markdown("""
    <details>
    <summary>‚ùì Guide d'utilisation</summary>
    
    **Nexus AI Assistant** est une plateforme IA multimodale compl√®te avec :
    
    - ü§ñ Support de 3 mod√®les IA puissants
    - üé® Analyse d'images avec Molmo 2 8B
    - üíæ Historique persistant des conversations
    - üì• Export en Markdown ou Texte
    - üé® Design premium avec interface n√©on
    - ‚ú® 100% Gratuit
    
    **Comment utiliser :**
    1. Cliquez sur "Nouveau Chat" pour d√©marrer
    2. Posez votre question
    3. Consultez l'historique dans la sidebar
    4. Exportez vos conversations
    
    </details>
    """, unsafe_allow_html=True)

# Contenu principal
if st.session_state.current_conversation is None:
    st.markdown("""
    # üåå **Nexus AI Assistant**
    
    ## Bienvenue dans Nexus AI Assistant
    
    Une plateforme IA multimodale compl√®te avec :
    
    - ‚úÖ Support de 3 mod√®les IA puissants
    - ‚úÖ Analyse d'images avec Molmo 2 8B
    - ‚úÖ Historique persistant des conversations
    - ‚úÖ Export en Markdown ou Texte
    - ‚úÖ Design premium avec interface n√©on
    - ‚úÖ 100% Gratuit
    
    **Commencez** en cliquant sur "Nouveau Chat" dans la sidebar ! üöÄ
    """)
else:
    conv = st.session_state.conversations[st.session_state.current_conversation]
    
    # Header
    col1, col2, col3 = st.columns([2, 2, 1])
    
    with col1:
        new_title = st.text_input("Titre", value=conv["title"], label_visibility="collapsed")
        if new_title != conv["title"]:
            conv["title"] = new_title
    
    with col2:
        model = st.selectbox(
            "Mod√®le",
            ["DeepSeek Chat", "Molmo 2 8B", "Llama 2 70B"],
            index=["DeepSeek Chat", "Molmo 2 8B", "Llama 2 70B"].index(conv["model"]),
            label_visibility="collapsed"
        )
        if model != conv["model"]:
            conv["model"] = model
            conv["messages"] = []
    
    with col3:
        if st.button("üì• Export", use_container_width=True):
            # G√©n√©rer le contenu d'export
            export_content = f"# {conv['title']}\n\n"
            export_content += f"**Mod√®le:** {conv['model']}\n"
            export_content += f"**Date:** {conv['created_at']}\n\n"
            export_content += "---\n\n"
            
            for msg in conv["messages"]:
                if msg["role"] == "user":
                    export_content += f"**Vous:** {msg['content']}\n\n"
                else:
                    export_content += f"**IA:** {msg['content']}\n\n"
            
            # Cr√©er le fichier
            st.download_button(
                label="T√©l√©charger en Markdown",
                data=export_content,
                file_name=f"{conv['title']}.md",
                mime="text/markdown"
            )
    
    st.markdown("---")
    
    # Affichage des messages
    if conv["messages"]:
        for msg in conv["messages"]:
            if msg["role"] == "user":
                st.markdown(f'<div class="message-box user-message"><b>Vous:</b> {msg["content"]}</div>', unsafe_allow_html=True)
            else:
                st.markdown(f'<div class="message-box ai-message"><b>IA:</b> {msg["content"]}</div>', unsafe_allow_html=True)
    else:
        st.info("Aucun message pour le moment. Posez votre premi√®re question !")
    
    st.markdown("---")
    
    # Zone de saisie
    st.markdown("### Votre message")
    message = st.text_area("Posez votre question...", height=100, label_visibility="collapsed")
    
    if st.button("üì§ Envoyer", use_container_width=True):
        if message.strip():
            # Ajouter le message utilisateur
            conv["messages"].append({
                "role": "user",
                "content": message
            })
            
            # Pr√©parer les messages pour l'API
            api_messages = [{"role": msg["role"], "content": msg["content"]} for msg in conv["messages"]]
            
            # Appeler l'API
            with st.spinner("‚è≥ Traitement en cours..."):
                response = call_claude_api(api_messages, conv["model"], temperature, max_tokens)
            
            if response:
                # Ajouter la r√©ponse IA
                conv["messages"].append({
                    "role": "assistant",
                    "content": response
                })
                st.rerun()
            else:
                st.error("‚ùå Erreur lors du traitement. Veuillez r√©essayer.")
